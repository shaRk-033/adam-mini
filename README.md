experimentation with new optimizer, [adam-mini](https://arxiv.org/abs/2406.16793).

training a simple gpt2 based arch with slight modifications like RoPE embed, SwiGLU, Grouped Query Attention with the Adam mini optimizer.
